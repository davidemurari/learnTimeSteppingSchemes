{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implicit Euler**\n",
    "$$\n",
    "\\min_{z\\in\\mathbb{R}^{51}}\\left\\|z-y_n-hLz\\right\\|_{2}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\partial_t u = \\mathcal{L}u\n",
    "$$\n",
    "$$\n",
    "\\dot{u}(t) = Lu(t)\\in\\mathbb{R}^{51}\n",
    "$$\n",
    "$$\n",
    "\\mathbb{R}\\ni u_i(t) = \\sum_{j=1}^H \\alpha_i^j \\varphi_j(t),\\,\\,i=1,...,51\n",
    "$$\n",
    "$$\n",
    "\\dot{u}_i(t_c) = \\sum_{j=1}^H \\alpha_i^j \\dot{\\varphi}_j(t_c),\\,\\,c=1,...,C\n",
    "$$\n",
    "$$\n",
    "\\dot{u}(t_c) = Lu(t_c)\n",
    "$$\n",
    "$$\n",
    "H_d\\alpha_i = (Lu(t))_i,\\,\\,\\alpha_i=[\\alpha_i^1,...,\\alpha_i^C]\\in\\mathbb{R}^C,\\,H_d\\in\\mathbb{R}^{C\\times H}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost linear system euler : $\\mathcal{O}(51^3)$\n",
    "Cost linear system ELM : $\\mathcal{O}(51\\cdot c)$\n",
    "\n",
    "where $c$ is the cost of solving the linear system for a single component of the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.38877855, -0.44174312, -0.34074649],\n",
       "       [-0.38877855, -0.44174312, -0.34074649]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y0 = np.random.randn(3,1)\n",
    "oo = np.ones((2,1))\n",
    "oo@y0.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(I\\otimes (hd))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELM method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W\\in\\mathbb{R}^{H\\times 51}\n",
    "$$\n",
    "$$\n",
    "y(t) = y0 + (h-h0)W\n",
    "$$\n",
    "$$\n",
    "h = [\\sigma(at_i+b)]_{i=1,...,n}\\in\\mathbb{R}^{n\\times H},\\,\\,h_0 = \\sigma(at_0+b)\n",
    "$$\n",
    "$$\n",
    "W \\in\\mathbb{R}^{H\\times d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{vec}(W)\\in\\mathbb{R}^{Hd},\\,\\,H\\in\\mathbb{R}^{nh},\\,\\,\\mathrm{vec}(Y)\\in\\mathbb{R}^{dn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r(t) = F(y(t))-\\dot{y}(t) \n",
    "$$\n",
    "$$\n",
    "\\partial_W r(t) = F'(y(t))(h-h0)-(hd)\n",
    "$$\n",
    "$$\n",
    "hd = a\\sigma'(at+b)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{vec}(r(t))}{\\partial \\mathrm{vec}(W)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{vec}(y(t)) = \\mathrm{vec}(y0) + (I_d\\otimes(h-h0))\\mathrm{vec}(W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial r(t)}{\\partial\\mathrm{vec}(W)} = F'(y(t))(I\\otimes (h-h0)) - \\mathrm{vec}(h_d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(y) = -y\\odot (D_1 y) + D_2 y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(Y) = -Y\\odot (YD_1^T) + YD_2^T\n",
    "$$\n",
    "$$\n",
    "\\mathrm{vec}(F(Y)) = -\\mathrm{vec}(Y)\\odot \\mathrm{vec}(YD_1^T)+(D_2\\otimes I_n)\\mathrm{vec}(Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{vec}(YD_1^T) = (D_1\\otimes I_n)\\mathrm{vec}(Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\mathrm{vec}(F(Y))}{\\mathrm{vec}(Y)} = - \\mathrm{diag}(\\mathrm{vec}(YD_1^T)) - \\mathrm{diag}(\\mathrm{vec}(Y))(D_1\\otimes I_n) + (D_2\\otimes I_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\mathrm{vec}(Y)}{\\partial\\mathrm{vec}(W)} = (I_d\\otimes (H-H_0))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(D_1\\otimes I_n)v = \\mathrm{vec}(VD_1),\\,\\,\\mathrm{vec}(V)=v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H\\in\\mathbb{R}^{n\\times H}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27738909, -2.03910191],\n",
       "       [ 0.55477818, -4.07820382],\n",
       "       [ 0.83216728, -6.11730573]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([1,2.,3])\n",
    "np.outer(v,np.random.randn(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(dn,dn) * (dn,dH) = (dn,dH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\mathrm{vec}(F(Y))}{\\partial\\mathrm{vec}(W)} = \\frac{\\partial\\mathrm{vec}(F(Y))}{\\partial\\mathrm{vec}(Y)}(I_d\\otimes (H-H_0))\n",
    "$$\\partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F'(y) = -\\mathrm{diag}(D_1y) - \\mathrm{diag}(y)D_1 + D_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.func import jacrev\n",
    "import numpy as np\n",
    "\n",
    "d = 10\n",
    "H = 30\n",
    "n = 20\n",
    "\n",
    "dtype = torch.float64\n",
    "\n",
    "Y0 = torch.rand((n,d),dtype=dtype)\n",
    "h = torch.rand((n,H),dtype=dtype)\n",
    "h0 = torch.rand((n,H),dtype=dtype)\n",
    "W = torch.rand((H,d),dtype=dtype)\n",
    "W.requires_grad =True\n",
    "\n",
    "D1 = torch.randn((d,d),dtype=dtype)\n",
    "D2 = torch.randn((d,d),dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "YY = lambda W : Y0 + (h-h0)@W\n",
    "F = lambda W : - YY(W.reshape(H,d)) * (YY(W.reshape(H,d))@D1.T) + YY(W) @ D2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_jac = (jacrev(F)(W)).detach().cpu().numpy().reshape((-1,H*d),order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = YY(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dy = - torch.diag_embed((Y@D1.T).T.reshape(-1)) - torch.diag_embed(Y.T.reshape(-1))@torch.kron(D1,torch.eye(n)) + torch.kron(D2,torch.eye(n))\n",
    "dy_dw = torch.kron(torch.eye(d),h-h0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dw = (df_dy @ dy_dw).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(df_dw, auto_jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1085951 ,  0.52624065, -0.81295227, -0.        ,  0.        ,\n",
       "        -0.        ],\n",
       "       [-0.        ,  0.        , -0.        , -0.1085951 ,  0.52624065,\n",
       "        -0.81295227]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = 2\n",
    "n = 3\n",
    "np.kron(np.eye(2),np.random.randn(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Burgers to make it faster and check it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dynamics import vecField\n",
    "from scripts.utils import act\n",
    "from scripts.dynamics import vecField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import diags, eye, kron\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flowMap:\n",
    "    def __init__(self,y0,initial_proj,weight,bias,dt=1,n_t=5,n_x=10,L=10,LB=-1.,UB=1.,system=\"Rober\",act_name=\"Tanh\",verbose=False):\n",
    "        \n",
    "        self.system = system\n",
    "        self.vec = vecField(system)\n",
    "        self.act = lambda t,w,b : act(t,w,b,act_name=act_name)\n",
    "        self.dt = dt\n",
    "        self.d = len(y0) #dimension phase space\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.n_x = n_x\n",
    "        self.h = np.zeros((n_x,L))\n",
    "        self.hd = np.zeros((n_x,L))\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "        self.y0 = y0\n",
    "        self.y0_supp = y0\n",
    "        \n",
    "        self.L = L #number of neurons\n",
    "        self.LB = LB #Lower boundary for weight and bias sampling \n",
    "        self.UB = UB #Upper boundary for weight and bias sampling\n",
    "        \n",
    "        if len(weight)==0:\n",
    "            self.weight = np.random.uniform(low=self.LB,high=self.UB,size=(self.L))\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        if len(bias)==0:\n",
    "            self.bias = np.random.uniform(low=self.LB,high=self.UB,size=(self.L))\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        \n",
    "        self.n_t = n_t\n",
    "        self.t_tot = np.linspace(0,dt,self.n_t)\n",
    "        self.x = np.linspace(0,dt,self.n_x)\n",
    "        \n",
    "        pattern = np.diag(np.ones(len(y0)))\n",
    "        vec_up = np.concatenate((np.array([0.]),np.ones(len(y0)-2)))\n",
    "        vec_down = np.concatenate((np.ones(len(y0)-2),np.array([0.])))\n",
    "        mat = pattern + np.diag(vec_up,1) + np.diag(vec_down,-1)\n",
    "        self.sparsity_pattern = np.kron(mat,np.ones((n_x,L)))\n",
    "\n",
    "        for i in range(n_x):\n",
    "            self.h[i], self.hd[i] = self.act(self.x[i],self.weight,self.bias)\n",
    "\n",
    "        self.h0 = self.h[0] #at the initial time, i.e. at x=0.\n",
    "        self.hf = self.h[-1]\n",
    "        self.hd0 = self.hd[0]\n",
    "        self.hdf = self.hd[-1]\n",
    "        \n",
    "        self.computational_time = None\n",
    "        self.computed_projection_matrices = np.tile(initial_proj,(self.n_t-1,1)) #one per time subintreval\n",
    "        self.computed_initial_conditions = np.zeros((self.n_t-1,self.d)) #one per time subinterval\n",
    "        self.training_err_vec = np.zeros((self.n_t,1))\n",
    "        self.sol = np.zeros((self.n_t,self.d))\n",
    "        \n",
    "    \n",
    "    def to_mat(self,w):    \n",
    "        return w.reshape((self.L,self.d),order='F') #order F means it reshapes by columns\n",
    "    \n",
    "    def residual(self,c_i,xi_i):\n",
    "        y = (self.h-self.h0)@self.to_mat(xi_i) + self.y0_supp.reshape(1,-1)\n",
    "        vecValue = self.vec.eval(y)\n",
    "        y_dot = c_i * self.hd @ self.to_mat(xi_i)\n",
    "        Loss = (y_dot-vecValue)\n",
    "        return Loss.reshape(-1,order='F')\n",
    "    \n",
    "    def re(self,a,H):\n",
    "        return np.einsum('i,ij->ij',a,H)\n",
    "    \n",
    "    def diag_embed(self,x):\n",
    "        a,b = x.shape\n",
    "        mat = np.zeros((a,b,b))\n",
    "        np.einsum('ijj->ij',mat)[:] = x[:]\n",
    "        return mat #provides a diagonal embedding of a batch of vectors\n",
    "    \n",
    "    def jac_residual(self,c_i,xi_i):\n",
    "        \n",
    "        y = (np.einsum('jk,ik->ij',self.h-self.h0,self.to_mat(xi_i).T) + self.y0_supp.reshape(-1,1)).T\n",
    "        H = self.h - self.h0        \n",
    "        dx, nu, N = self.vec.dx, self.vec.nu, self.vec.N\n",
    "        n,d = y.shape\n",
    "        \n",
    "        vv = np.ones(N-1)\n",
    "        Shift_forward = diags([vv], [1], shape=(N, N))#np.diag(vv,k=1)\n",
    "        Shift_backward = diags([vv], [-1], shape=(N, N))#np.diag(vv,k=-1)\n",
    "        \n",
    "        Shift_backward = Shift_backward.tolil()\n",
    "        Shift_backward[-1, -2] = 0  # Adjust for sparse matrix indexing\n",
    "        Shift_forward = Shift_forward.tolil()\n",
    "        Shift_forward[0, 1] = 0\n",
    "        Shift_backward = Shift_backward.tocsr()\n",
    "        Shift_forward = Shift_forward.tocsr()\n",
    "        \n",
    "        #For the boundary conditions\n",
    "        #Shift_backward[-1]*=0\n",
    "        #Shift_forward[0]*=0\n",
    "        \n",
    "        D2 = (Shift_forward + Shift_backward - 2*np.eye(N))/(dx**2)\n",
    "        D1 = 1/(2*dx) * (Shift_forward-Shift_backward)\n",
    "        \n",
    "        #df_dy = -np.diag((y@D1.T).reshape(-1,order='F')) - np.diag(y.reshape(-1,order='F'))@np.kron(D1,np.eye(n)) + np.kron(D2,np.eye(n))\n",
    "        df_dy = -sparse.diags((y @ D1.T).reshape(-1, order='F')) - sparse.diags(y.reshape(-1, order='F')) @ kron(D1, eye(n)) + kron(D2, eye(n))\n",
    "        #dy_dw = np.kron(np.eye(d),H)\n",
    "        dy_dw = kron(eye(d),H)\n",
    "        jj = -df_dy@dy_dw\n",
    "        a = d // n\n",
    "        #for i in range(a):\n",
    "        #    jj[i*self.n_x:(i+1)*n,i*n:(i+1)*n] += c_i*self.hd\n",
    "        return jj.todense()\n",
    "\n",
    "    def approximate_flow_map(self,IterMax=100,IterTol=1e-5):\n",
    "        \n",
    "        self.training_err_vec[0] = 0.        \n",
    "        self.sol[0] = self.y0_supp\n",
    "        \n",
    "        for i in range(self.n_t-1):\n",
    "            \n",
    "            self.iter = 1 #Just added\n",
    "            \n",
    "            c_i = self.dt / (self.t_tot[i+1]-self.t_tot[i])\n",
    "            xi_i = self.computed_projection_matrices[i] \n",
    "            Loss = self.residual(c_i,xi_i)\n",
    "            l2 = [2.,1]\n",
    "            \n",
    "            self.sparsity_pattern = 1.-1.*(self.jac_residual(c_i,xi_i)==0.)\n",
    "            \n",
    "            self.computed_initial_conditions[i] = self.y0_supp\n",
    "                \n",
    "            '''while np.abs(l2[1])>IterTol and self.iter<IterMax and np.abs(l2[0]-l2[1])>IterTol:\n",
    "                \n",
    "                l2[0] = l2[1] #this says that we suppose to converge after this block of code runs\n",
    "                JJ = self.jac_residual(c_i,xi_i)\n",
    "                print((1.-(JJ==0)).sum())\n",
    "                dxi = np.linalg.lstsq(JJ,Loss,rcond=None)[0]\n",
    "                xi_i = xi_i - dxi\n",
    "                Loss = self.residual(c_i,xi_i)\n",
    "                l2[1] = np.linalg.norm(Loss,ord=2) #This is used as a stopping criterion\n",
    "                \n",
    "                self.iter+=1'''\n",
    "            \n",
    "            func = lambda x : self.residual(c_i,x)\n",
    "            print(xi_i.shape)\n",
    "            xi_i = least_squares(func,x0=self.computed_projection_matrices[i],verbose=2,xtol=1e-2).x#,jac_sparsity=self.sparsity_pattern).x\n",
    "            '''#Store the computed result\n",
    "            for i in range(51):\n",
    "                for j in range(51):\n",
    "                    print(f\"i {i}, j {j}\")\n",
    "                    print(xi_i.jac[i*(self.n_x):(i+1)*(self.n_x),j*(self.L):(j+1)*(self.L)])\n",
    "            xi_i = xi_i.x'''\n",
    "            self.computed_projection_matrices[i] = xi_i\n",
    "            \n",
    "            y = (np.einsum('jk,ik->ij',self.h-self.h0,xi_i.reshape(len(self.y0),self.L)) + self.y0_supp.reshape(-1,1)).T\n",
    "            self.y0_supp = y[-1]\n",
    "            self.sol[i+1] = self.y0_supp\n",
    "            \n",
    "            self.training_err_vec[i+1] = np.sqrt(np.mean(Loss**2))\n",
    "\n",
    "    def analyticalApproximateSolution(self,t):\n",
    "        j = np.searchsorted(self.t_tot,t,side='left') #determines the index of the largest number in t_tot that is smaller than t\n",
    "        #In other words, it finds where to place t in t_tot in order to preserve its increasing ordering\n",
    "          \n",
    "        y_0 = self.sol[max(0,j-1)]        \n",
    "        jp = 1 if j==0 else j\n",
    "        x = np.array([(t - self.t_tot[max(0,j-1)]) / (self.t_tot[jp]-self.t_tot[max(0,j-1)])])\n",
    "        \n",
    "        h,_ = act(x,self.weight,self.bias)\n",
    "        h0,_ = act(0*x,self.weight,self.bias)\n",
    "        y = (h-h0)@self.to_mat(self.computed_projection_matrices[max(0,j-1)]) + y_0\n",
    "        return y\n",
    "    \n",
    "    def plotOverTimeRange(self,time):\n",
    "        sol_approximation = np.zeros((self.d,len(time)))\n",
    "        for i,t in enumerate(time):\n",
    "            sol_approximation[:,i] = self.analyticalApproximateSolution(t)\n",
    "        return sol_approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "system=\"Burger\"\n",
    "vecRef = vecField(system=system)\n",
    "nu = vecRef.nu\n",
    "dx = vecRef.dx\n",
    "y0 = np.sin(2*np.pi*vecRef.x)\n",
    "t_max = 1.\n",
    "L = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2550,)\n",
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         5.9717e+04                                    2.61e+04    \n",
      "       1              2         3.9302e+03      5.58e+04       5.05e+01       1.29e+04    \n",
      "       2              3         1.3583e+02      3.79e+03       1.01e+02       1.05e+03    \n",
      "       3              4         5.7375e+01      7.85e+01       2.02e+02       6.86e+02    \n",
      "       4              5         2.9881e+00      5.44e+01       4.04e+02       1.56e+02    \n",
      "       5              6         9.3067e-02      2.90e+00       8.08e+02       2.72e+01    \n",
      "       6              7         4.4919e-02      4.81e-02       1.62e+03       1.75e+01    \n",
      "       7              8         1.0437e-02      3.45e-02       3.23e+03       9.42e-01    \n",
      "       8             10         1.5169e-04      1.03e-02       1.62e+03       1.13e+00    \n",
      "       9             13         5.6449e-06      1.46e-04       2.02e+02       5.95e-02    \n",
      "      10             16         7.0056e-07      4.94e-06       2.52e+01       8.02e-03    \n",
      "      11             18         1.5598e-08      6.85e-07       1.26e+01       2.01e-03    \n",
      "      12             21         1.0694e-09      1.45e-08       1.58e+00       3.44e-04    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 21, initial cost 5.9717e+04, final cost 1.0694e-09, first-order optimality 3.44e-04.\n",
      "(2550,)\n",
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         6.3491e+04                                    1.78e+04    \n",
      "       1              2         1.3254e+03      6.22e+04       5.05e+01       5.92e+03    \n",
      "       2              3         2.5611e+01      1.30e+03       1.01e+02       4.47e+02    \n",
      "       3              4         1.9221e+01      6.39e+00       2.02e+02       3.89e+02    \n",
      "       4              5         1.0133e+00      1.82e+01       4.04e+02       9.54e+01    \n",
      "       5              6         6.7733e-02      9.46e-01       8.08e+02       2.41e+01    \n",
      "       6              7         6.3440e-02      4.29e-03       1.62e+03       1.98e+01    \n",
      "       7              8         2.1916e-02      4.15e-02       3.23e+03       2.78e+00    \n",
      "       8             10         9.7973e-05      2.18e-02       1.62e+03       8.56e-01    \n",
      "       9             14         7.2360e-08      9.79e-05       5.05e+01       4.43e-02    \n",
      "      10             18         2.4278e-09      6.99e-08       1.58e+00       7.17e-04    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 18, initial cost 6.3491e+04, final cost 2.4278e-09, first-order optimality 7.17e-04.\n",
      "(2550,)\n",
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         5.1142e+04                                    1.59e+04    \n",
      "       1              2         3.8343e+02      5.08e+04       5.05e+01       3.13e+03    \n",
      "       2              3         4.0217e+00      3.79e+02       1.01e+02       1.72e+02    \n",
      "       3              4         5.1815e-01      3.50e+00       2.02e+02       6.32e+01    \n",
      "       4              5         4.6200e-01      5.61e-02       4.04e+02       5.97e+01    \n",
      "       5              6         2.5941e-03      4.59e-01       8.08e+02       3.64e+00    \n",
      "       6              8         2.5091e-06      2.59e-03       4.04e+02       9.96e-02    \n",
      "       7             12         2.0438e-08      2.49e-06       1.26e+01       2.04e-03    \n",
      "       8             15         1.7917e-09      1.86e-08       1.58e+00       3.52e-04    \n",
      "       9             17         1.8983e-11      1.77e-09       7.89e-01       2.07e-04    \n",
      "      10             19         1.8983e-11      0.00e+00       0.00e+00       2.07e-04    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 19, initial cost 5.1142e+04, final cost 1.8983e-11, first-order optimality 2.07e-04.\n",
      "(2550,)\n",
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         4.5781e+04                                    1.50e+04    \n",
      "       1              2         1.1800e+02      4.57e+04       5.05e+01       1.91e+03    \n",
      "       2              3         5.2038e+01      6.60e+01       1.01e+02       1.27e+03    \n",
      "       3              4         4.5339e-01      5.16e+01       2.02e+02       8.88e+01    \n",
      "       4              5         2.7101e-03      4.51e-01       4.04e+02       3.90e+00    \n",
      "       5              7         2.4090e-04      2.47e-03       2.02e+02       1.36e-01    \n",
      "       6              9         2.5524e-06      2.38e-04       1.01e+02       6.22e-02    \n",
      "       7             12         9.2542e-09      2.54e-06       1.26e+01       1.74e-03    \n",
      "       8             15         4.8824e-10      8.77e-09       1.58e+00       2.65e-04    \n",
      "       9             17         4.2469e-10      6.36e-11       7.89e-01       1.61e-04    \n",
      "      10             19         4.9201e-13      4.24e-10       3.95e-01       4.82e-05    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 19, initial cost 4.5781e+04, final cost 4.9201e-13, first-order optimality 4.82e-05.\n"
     ]
    }
   ],
   "source": [
    "initial_proj = np.ones((len(y0)*L)).reshape(1,-1)\n",
    "weight = np.random.randn(L)\n",
    "bias = np.random.randn(L)\n",
    "flow = flowMap(y0,initial_proj,weight,bias,system=system,L=L)\n",
    "flow.approximate_flow_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import lsqr, LinearOperator as linOp\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import random,kron\n",
    "\n",
    "A = np.random.randn(10,1900)\n",
    "B = np.random.randn(70,200)\n",
    "b = np.random.randn(700)\n",
    "Mat = np.kron(A,B)\n",
    "\n",
    "\n",
    "d1,d2 = A.shape\n",
    "c1,c2 = B.shape\n",
    "\n",
    "#x_linalg = np.linalg.solve(Mat,b)\n",
    "\n",
    "def mv(v):\n",
    "    V = v.reshape((c2,d2),order='F')\n",
    "    return (B@V@A.T).reshape((-1,1),order='F')\n",
    "def rmv(v):\n",
    "    V = v.reshape((c1,d1),order='F')\n",
    "    return (B.T@V@A).reshape((-1,1),order='F')\n",
    "    \n",
    "M = linOp((c1*d1,c2*d2),matvec=mv,rmatvec=rmv)\n",
    "\n",
    "tt = time.time()\n",
    "x_linalg = np.linalg.lstsq(Mat,b,rcond=None)\n",
    "print(\"Time for linalg : \",time.time()-tt)\n",
    "\n",
    "tt = time.time()\n",
    "x_lsqr = lsqr(M,b,atol=1e-10,btol=1e-10,show=False)[0]\n",
    "print(\"Time for lsqr : \",time.time()-tt)\n",
    "np.allclose(Mat@x_lsqr,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the linear system for Burger's equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioning of h-h0 :  163306777647281.56\n",
      "Conditioning of the Jacobian matrix :  2.3485173553183004e+16\n",
      "Jacobian vector product :  True\n",
      "Jacobian transpose vector product :  True\n",
      "True shape :  (1530, 510)\n",
      "Computed shape  : (1530, 510)\n",
      "xx :  (510,)\n",
      "Time for linalg :  0.04984092712402344\n",
      "Time for lsqr :  0.6727242469787598\n",
      "Is lsqr correct?  False\n",
      "Is linalg correct?  True\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import lsqr, LinearOperator as linOp\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags, eye, kron\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse import random,kron\n",
    "\n",
    "N = 51\n",
    "dx = 1/(N-1)\n",
    "vv = np.ones(N-1)\n",
    "Shift_forward = diags([vv], [1], shape=(N, N))#np.diag(vv,k=1)\n",
    "Shift_backward = diags([vv], [-1], shape=(N, N))#np.diag(vv,k=-1)\n",
    "\n",
    "Shift_backward = Shift_backward.tolil()\n",
    "Shift_backward[-1, -2] = 0  # Adjust for sparse matrix indexing\n",
    "Shift_forward = Shift_forward.tolil()\n",
    "Shift_forward[0, 1] = 0\n",
    "Shift_backward = Shift_backward.tocsr()\n",
    "Shift_forward = Shift_forward.tocsr()\n",
    "\n",
    "D2 = (Shift_forward + Shift_backward - 2*np.eye(N))/(dx**2)\n",
    "D1 = 1/(2*dx) * (Shift_forward-Shift_backward)\n",
    "\n",
    "n_x = 30\n",
    "L = 10\n",
    "\n",
    "h = np.zeros((n_x,L))\n",
    "hd = np.zeros((n_x,L))\n",
    "LB = -1.\n",
    "UB = 1.\n",
    "weight = np.random.uniform(low=LB,high=UB,size=(L))\n",
    "bias = np.random.uniform(low=LB,high=UB,size=(L))\n",
    "x = np.linspace(0,1.,n_x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def act(x,a,b):\n",
    "    return sigmoid(a*x+b), a*(sigmoid(a*x+b)*(1-sigmoid(a*x+b)**2))\n",
    "for i in range(n_x):\n",
    "    h[i], hd[i] = act(x[i],weight,bias)\n",
    "#h,hd of size n_x x L\n",
    "h0 = h[0:1]\n",
    "hd0 = hd[0:1]\n",
    "domain = np.linspace(0,1,N)\n",
    "y0 = np.sin(domain*2*np.pi).reshape(-1,1)\n",
    "\n",
    "W = np.random.randn(L,N)\n",
    "Y = np.ones((n_x,1))@y0.T + (h-h0)@W\n",
    "F = - Y * (Y@D1.T) + Y@D2.T\n",
    "Yd = hd@W\n",
    "\n",
    "#Dimension of the Jacobian is N n_x x N L\n",
    "#Thus the input vector v has shape N L \n",
    "#and the known vector for the linear system is of N n_x entries\n",
    "def vec(Y):\n",
    "    return Y.reshape((-1),order='F')\n",
    "def to_mat(y,a,b):\n",
    "    return y.reshape((a,b),order='F')\n",
    "\n",
    "J = (-np.diag(vec(Y@D1.T))-np.diag(vec(Y))@kron(D1,np.eye(n_x))+kron(D2,np.eye(n_x))) @ np.kron(np.eye(N),(h-h0)) - np.kron(np.eye(N),hd)\n",
    "\n",
    "print(\"Conditioning of h-h0 : \",np.linalg.cond(h))\n",
    "print(\"Conditioning of the Jacobian matrix : \",np.linalg.cond(J))\n",
    "\n",
    "def mv(v):\n",
    "    V = to_mat(v,L,N)\n",
    "    hV = (h-h0)@V\n",
    "    return vec(-(Y@D1.T)*hV-Y*((h-h0)@(V@D1.T))+(h-h0)@(V@D2.T)-hd@V)\n",
    "def rmv(v):\n",
    "    V = to_mat(v,n_x,N)\n",
    "    return vec((h-h0).T@[-(Y@D1.T)*V - (Y*V)@D1 + V@D2] - hd.T@V)\n",
    "\n",
    "v = np.random.randn(L*N)\n",
    "print(\"Jacobian vector product : \",np.allclose(mv(v),J@v))\n",
    "w = np.random.randn(n_x*N)\n",
    "print(\"Jacobian transpose vector product : \",np.allclose(rmv(w),J.T@w))\n",
    "\n",
    "shape = (n_x*N,N*L)\n",
    "M = linOp(shape,matvec=mv,rmatvec=rmv)\n",
    "\n",
    "b = np.ones((N*n_x))\n",
    "\n",
    "tt = time.time()\n",
    "x_linalg = np.linalg.lstsq(J,b,rcond=None)[0]\n",
    "\n",
    "print(\"True shape : \",J.shape)\n",
    "print(\"Computed shape  :\",shape)\n",
    "print(\"xx : \",x_linalg.shape)\n",
    "\n",
    "print(\"Time for linalg : \",time.time()-tt)\n",
    "\n",
    "tt = time.time()\n",
    "x_lsqr = lsqr(M,b,atol=1e-6,btol=1e-6,show=False)[0]\n",
    "\n",
    "print(\"Time for lsqr : \",time.time()-tt)\n",
    "\n",
    "print(\"Is lsqr correct? \",np.allclose(J@x_lsqr,b))\n",
    "print(\"Is linalg correct? \",np.allclose(J@x_linalg,b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
